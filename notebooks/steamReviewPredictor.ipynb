{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8ec6cad",
   "metadata": {},
   "source": [
    "Task 1: Data Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417322f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'language' found. Filtering by metadata...\n",
      "Saving filtered English reviews to ../data/filtered_reviews.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anya\\AppData\\Roaming\\Python\\Python312\\site-packages\\dask\\dataframe\\io\\csv.py:77: DtypeWarning: Columns (23) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = reader(bio, **kwargs)\n",
      "C:\\Users\\anya\\AppData\\Roaming\\Python\\Python312\\site-packages\\dask\\dataframe\\io\\csv.py:77: DtypeWarning: Columns (23) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = reader(bio, **kwargs)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Mismatched dtypes found in `pd.read_csv`/`pd.read_table`.\n\n+----------------------+--------+----------+\n| Column               | Found  | Expected |\n+----------------------+--------+----------+\n| steam_china_location | object | float64  |\n+----------------------+--------+----------+\n\nThe following columns also raised exceptions on conversion:\n\n- steam_china_location\n  ValueError(\"could not convert string to float: '广东'\")\n\nUsually this is due to dask's dtype inference failing, and\n*may* be fixed by specifying dtypes manually by adding:\n\ndtype={'steam_china_location': 'object'}\n\nto the call to `read_csv`/`read_table`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 61\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSaving filtered English reviews to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mOUTPUT_FILE\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# compute() triggers the actual processing and saving\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m \u001b[43mdf_english\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mOUTPUT_FILE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msingle_file\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFiltering Complete :D\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\dask\\dataframe\\dask_expr\\_collection.py:2444\u001b[39m, in \u001b[36mFrameBase.to_csv\u001b[39m\u001b[34m(self, filename, **kwargs)\u001b[39m\n\u001b[32m   2441\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"See dd.to_csv docstring for more information\"\"\"\u001b[39;00m\n\u001b[32m   2442\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdask\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdataframe\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcsv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m to_csv\n\u001b[32m-> \u001b[39m\u001b[32m2444\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\dask\\dataframe\\io\\csv.py:967\u001b[39m, in \u001b[36mto_csv\u001b[39m\u001b[34m(df, filename, single_file, encoding, mode, name_function, compression, compute, scheduler, storage_options, header_first_partition_only, compute_kwargs, **kwargs)\u001b[39m\n\u001b[32m    963\u001b[39m         compute_kwargs[\u001b[33m\"\u001b[39m\u001b[33mscheduler\u001b[39m\u001b[33m\"\u001b[39m] = scheduler\n\u001b[32m    965\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdask\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m967\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[43mdask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcompute_kwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    968\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    969\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m values\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\dask\\base.py:681\u001b[39m, in \u001b[36mcompute\u001b[39m\u001b[34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[39m\n\u001b[32m    678\u001b[39m     expr = expr.optimize()\n\u001b[32m    679\u001b[39m     keys = \u001b[38;5;28mlist\u001b[39m(flatten(expr.__dask_keys__()))\n\u001b[32m--> \u001b[39m\u001b[32m681\u001b[39m     results = \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    683\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m repack(results)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\dask\\dataframe\\io\\csv.py:351\u001b[39m, in \u001b[36m_read_csv\u001b[39m\u001b[34m(block, part, columns, reader, header, dtypes, head, colname, full_columns, enforce, kwargs, blocksize)\u001b[39m\n\u001b[32m    348\u001b[39m         rest_kwargs[\u001b[33m\"\u001b[39m\u001b[33musecols\u001b[39m\u001b[33m\"\u001b[39m] = _columns\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# Call `pandas_read_text`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m351\u001b[39m df = \u001b[43mpandas_read_text\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrest_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwrite_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43menforce\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m project_after_read:\n\u001b[32m    363\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df[columns]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\dask\\dataframe\\io\\csv.py:79\u001b[39m, in \u001b[36mpandas_read_text\u001b[39m\u001b[34m(reader, b, header, kwargs, dtypes, columns, write_header, enforce, path)\u001b[39m\n\u001b[32m     77\u001b[39m df = reader(bio, **kwargs)\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dtypes:\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     \u001b[43mcoerce_dtypes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtypes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m enforce \u001b[38;5;129;01mand\u001b[39;00m columns \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mlist\u001b[39m(df.columns) != \u001b[38;5;28mlist\u001b[39m(columns)):\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mColumns do not match\u001b[39m\u001b[33m\"\u001b[39m, df.columns, columns)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\dask\\dataframe\\io\\csv.py:180\u001b[39m, in \u001b[36mcoerce_dtypes\u001b[39m\u001b[34m(df, dtypes)\u001b[39m\n\u001b[32m    176\u001b[39m rule = \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m % (\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m61\u001b[39m)\n\u001b[32m    177\u001b[39m msg = \u001b[33m\"\u001b[39m\u001b[33mMismatched dtypes found in `pd.read_csv`/`pd.read_table`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m % (\n\u001b[32m    178\u001b[39m     rule.join(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, [dtype_msg, date_msg]))\n\u001b[32m    179\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[31mValueError\u001b[39m: Mismatched dtypes found in `pd.read_csv`/`pd.read_table`.\n\n+----------------------+--------+----------+\n| Column               | Found  | Expected |\n+----------------------+--------+----------+\n| steam_china_location | object | float64  |\n+----------------------+--------+----------+\n\nThe following columns also raised exceptions on conversion:\n\n- steam_china_location\n  ValueError(\"could not convert string to float: '广东'\")\n\nUsually this is due to dask's dtype inference failing, and\n*may* be fixed by specifying dtypes manually by adding:\n\ndtype={'steam_china_location': 'object'}\n\nto the call to `read_csv`/`read_table`."
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "from langdetect import detect, LangDetectException\n",
    "import pandas as pd\n",
    "\n",
    "# CONFIGURATION\n",
    "# Establish input and output files\n",
    "INPUT_FILE = '../data/weighted_score_above_08.csv' \n",
    "OUTPUT_FILE = '../data/filtered_reviews.csv'\n",
    "\n",
    "# STEP 1: LOAD DATA\n",
    "# Read the pre-filtered CSV that only contains reviews with helpfulness > 0.8\n",
    "# the columns we will be looking at\n",
    "cols_to_read = [\n",
    "    'language', \n",
    "    'review', \n",
    "    'voted_up', \n",
    "    'weighted_vote_score', \n",
    "    'votes_funny', \n",
    "    'steam_purchase', \n",
    "    'received_for_free', \n",
    "    'written_during_early_access', \n",
    "    'author_playtime_at_review', \n",
    "    'author_num_games_owned', \n",
    "    'author_num_reviews', \n",
    "    'game'\n",
    "]\n",
    "\n",
    "df = dd.read_csv(\n",
    "    INPUT_FILE, \n",
    "    usecols=cols_to_read,\n",
    "    dtype={\n",
    "        'votes_funny': 'float',\n",
    "        'weighted_vote_score': 'float',\n",
    "        'author_playtime_at_review': 'float',\n",
    "        'author_num_games_owned': 'float',\n",
    "        'author_num_reviews': 'float',\n",
    "        'voted_up': 'object'       # Reads True/False safely\n",
    "    },\n",
    "    quotechar='\"', \n",
    "    doublequote=True,\n",
    "    on_bad_lines='skip'\n",
    ")\n",
    "\n",
    "# STEP 2: FILTER FOR ENGLISH\n",
    "# use the language column in the steam dataset to filter to english-only reviews\n",
    "# will remove this column later to reduce redundancy\n",
    "if 'language' in df.columns:\n",
    "    print(\"Column 'language' found. Filtering by metadata...\")\n",
    "    df_english = df[df['language'] == 'english']\n",
    "else:\n",
    "    raise ValueError(\"The 'language' column was not found in the dataset\")\n",
    "\n",
    "# STEP 3: CLEANUP TEXT\n",
    "# to handle unusaly line terminator issue \n",
    "df_english['review'] = df_english['review'].str.replace(r'[\\n\\r]+', ' ', regex=True)\n",
    "df_english['review'] = df_english['review'].str.strip()\n",
    "\n",
    "# STEP 4: SELECT COLUMNS\n",
    "# Ensure we only keep available columns (prevents errors if 'game' is missing)\n",
    "final_columns = [c for c in cols_to_read if c != 'language']\n",
    "df_final = df_english[final_columns]\n",
    "\n",
    "# STEP 5: SAVE\n",
    "# Drop NaNs just before saving\n",
    "df_final = df_final.dropna(subset=['review'])\n",
    "\n",
    "print(f\"Saving filtered English reviews to {OUTPUT_FILE}...\")\n",
    "\n",
    "# compute() triggers the actual processing and saving\n",
    "df_english.to_csv(OUTPUT_FILE, index=False, single_file=True)\n",
    "\n",
    "print(\"Filtering Complete :D\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
